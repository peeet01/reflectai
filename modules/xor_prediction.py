import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, log_loss
import plotly.graph_objects as go


def run():
    st.set_page_config(layout="wide")
    st.title("ğŸ”€ XOR PredikciÃ³ â€“ TÃ¶bbrÃ©tegÅ± Perceptron")

    # ğŸ§­ BevezetÃ©s
    st.markdown("""
    A klasszikus **XOR logikai feladat** nem oldhatÃ³ meg egyrÃ©tegÅ± perceptronnal,  
    viszont egy **rejtett rÃ©teggel** ellÃ¡tott MLP kÃ©pes megtanulni.  
    Ez a modul vizualizÃ¡lja a tanulÃ¡si folyamatot, a dÃ¶ntÃ©si felÃ¼letet, Ã©s a sÃºlystruktÃºrÃ¡t.
    """)

    # ğŸ›ï¸ ParamÃ©terek
    st.sidebar.header("ğŸšï¸ ParamÃ©terek")
    hidden_size = st.sidebar.slider("Rejtett rÃ©teg mÃ©rete", 2, 10, 4)
    learning_rate = st.sidebar.slider("TanulÃ¡si rÃ¡ta", 0.001, 0.1, 0.01, step=0.001)
    max_iter = st.sidebar.slider("Max iterÃ¡ciÃ³", 100, 2000, 500, step=100)
    solver = st.sidebar.selectbox("Solver", ["adam", "sgd", "lbfgs"])
    activation = st.sidebar.selectbox("AktivÃ¡ciÃ³s fÃ¼ggvÃ©ny", ["relu", "logistic", "tanh"])
    alpha = st.sidebar.slider("RegulÃ¡rizÃ¡ciÃ³s erÅ‘ (alpha)", 0.0001, 0.1, 0.001, step=0.0001)

    # ğŸ§± XOR adat
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])

    # ğŸ§  HÃ¡lÃ³ lÃ©trehozÃ¡s + tanÃ­tÃ¡s
    model = MLPClassifier(hidden_layer_sizes=(hidden_size,),
                          learning_rate_init=learning_rate,
                          max_iter=max_iter,
                          solver=solver,
                          activation=activation,
                          alpha=alpha,
                          random_state=42)
    model.fit(X, y)
    preds = model.predict(X)
    proba = model.predict_proba(X)[:, 1]
    acc = accuracy_score(y, preds)
    loss = log_loss(y, proba)

    # ğŸ“‰ 2D dÃ¶ntÃ©si fÃ¼ggvÃ©ny
    st.subheader("ğŸ“ˆ DÃ¶ntÃ©si felÃ¼let (2D)")
    xx, yy = np.meshgrid(np.linspace(0, 1, 200), np.linspace(0, 1, 200))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = model.predict(grid).reshape(xx.shape)
    plt.figure(figsize=(4, 4))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap="RdBu")
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap="RdBu", edgecolor="k", s=100)
    plt.title(f"PontossÃ¡g: {acc * 100:.1f}%, VesztesÃ©g: {loss:.4f}")
    st.pyplot(plt.gcf())

    # ğŸŒ 3D aktivÃ¡lÃ¡sok
    st.subheader("ğŸŒ Rejtett rÃ©teg aktivÃ¡ciÃ³ (3D)")
    act = model.predict_proba(grid)[:, 1].reshape(xx.shape)
    fig3d = go.Figure(data=[go.Surface(z=act, x=xx, y=yy, colorscale="Viridis")])
    fig3d.update_layout(
        scene=dict(xaxis_title="xâ‚", yaxis_title="xâ‚‚", zaxis_title="P(rejtett=1)"),
        margin=dict(l=10, r=10, t=50, b=10),
        height=600)
    st.plotly_chart(fig3d, use_container_width=True)

    # ğŸ§© EredmÃ©ny
    st.subheader("ğŸ¯ EredmÃ©nyek")
    st.markdown(f"""
    - HÃ¡lÃ³ struktÃºrÃ¡ja: **Inputâ€“{hidden_size}â€“Output**  
    - AktivÃ¡ciÃ³: **{activation}**  
    - Solver: **{solver}**  
    - TanulÃ¡si rÃ¡ta: **{learning_rate}**  
    - RegulÃ¡rizÃ¡ciÃ³ (alpha): **{alpha}**  
    - IterÃ¡ciÃ³: **{model.n_iter_} / {max_iter}**  
    - PontossÃ¡g: **{acc * 100:.2f}%**  
    - Log-loss: **{loss:.5f}**
    """)

    # ğŸ“ CSV export
    st.subheader("ğŸ’¾ SÃºlyok exportÃ¡lÃ¡sa CSV-ben")
    weight_vectors = np.concatenate([w.flatten() for w in model.coefs_])
    df = pd.DataFrame([weight_vectors], columns=[f"w{i}" for i in range(len(weight_vectors))])
    csv = df.to_csv(index=False).encode('utf-8')
    st.download_button("â¬‡ï¸ SÃºlyok letÃ¶ltÃ©se", data=csv, file_name="xor_weights.csv")

    # ğŸ“˜ TudomÃ¡nyos hÃ¡ttÃ©r â€“ Latex
    st.markdown("### ğŸ“™ TudomÃ¡nyos hÃ¡ttÃ©r")
    st.latex(r"""
    \text{XOR kimenet:}\quad
    y = x_1 \oplus x_2 = x_1(1 - x_2) + x_2(1 - x_1)
    """)
    st.latex(r"""
    \text{MLP kimenet:}\quad
    \hat y = \sigma^{(2)}\left(W^{(2)} \cdot \sigma^{(1)}(W^{(1)}x + b^{(1)}) + b^{(2)}\right)
    """)
    st.latex(r"""
    \text{CÃ©lfÃ¼ggvÃ©ny:}\quad
    \mathcal{L} = -\frac{1}{N}\sum_{i=1}^N \left[ y_i \log \hat y_i + (1 - y_i) \log(1 - \hat y_i) \right]
    """)
    st.markdown("""
    A **tÃ¶bbrÃ©tegÅ± perceptron** (MLP) kÃ©pes a nemlineÃ¡ris elvÃ¡lasztÃ¡si problÃ©mÃ¡k â€“ mint az XOR â€“ megoldÃ¡sÃ¡ra.  
    A hÃ¡lÃ³ megtanulja lekÃ©pezni a nemlineÃ¡ris dÃ¶ntÃ©si hatÃ¡rt, a **log-loss** pedig a predikciÃ³k biztonsÃ¡gÃ¡t mÃ©ri.

    A paramÃ©terek vÃ¡ltoztatÃ¡sÃ¡val tanulmÃ¡nyozhatjuk:
    - a **konvergencia** sebessÃ©gÃ©t
    - a **tÃºlillesztÃ©s** kockÃ¡zatÃ¡t (alacsony alpha Ã©rtÃ©knÃ©l)
    - a kÃ¼lÃ¶nbÃ¶zÅ‘ aktivÃ¡ciÃ³s fÃ¼ggvÃ©nyek viselkedÃ©sÃ©t

    Ez a modul lehetÅ‘sÃ©get ad mÃ©lyebb **tanulÃ¡sdinamika Ã©s hÃ¡lÃ³architektÃºra** vizsgÃ¡latra is.
    """)


# ReflectAI kompatibilitÃ¡s
app = run
