import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, log_loss
import plotly.graph_objects as go

def run():
    st.set_page_config(layout="wide")
    st.title("ğŸ”€ XOR PredikciÃ³ â€“ TÃ¶bbrÃ©tegÅ± Perceptron")

    # ğŸ§­ BevezetÃ©s
    st.markdown("""
    A klasszikus **XOR logikai feladat** nem oldhatÃ³ meg egyrÃ©tegÅ± perceptronnal,  
    viszont egy **rejtett rÃ©teggel** ellÃ¡tott MLP kÃ©pes megtanulni.  
    Ez a modul vizualizÃ¡lja a tanulÃ¡si folyamatot, sÃºlyokat, dÃ¶ntÃ©si felÃ¼letet Ã©s az aktivÃ¡ciÃ³t.
    """)

    # ğŸ›ï¸ ParamÃ©terek
    st.sidebar.header("ğŸšï¸ ParamÃ©terek")
    hidden_size = st.sidebar.slider("Rejtett rÃ©teg mÃ©rete", 2, 10, 4)
    learning_rate = st.sidebar.slider("TanulÃ¡si rÃ¡ta", 0.001, 0.1, 0.01, step=0.001)
    max_iter = st.sidebar.slider("Max iterÃ¡ciÃ³", 100, 2000, 500, step=100)
    solver = st.sidebar.selectbox("Solver", ["adam", "sgd", "lbfgs"])

    # ğŸ§± XOR adat
    X = np.array([[0,0],[0,1],[1,0],[1,1]])
    y = np.array([0,1,1,0])

    # ğŸ§  HÃ¡lÃ³ lÃ©trehozÃ¡s + tanÃ­tÃ¡s
    model = MLPClassifier(
        hidden_layer_sizes=(hidden_size,),
        learning_rate_init=learning_rate,
        max_iter=max_iter,
        solver=solver,
        random_state=42,
        verbose=False
    )
    model.fit(X, y)
    preds = model.predict(X)
    probs = model.predict_proba(X)[:,1]
    acc = accuracy_score(y, preds)

    # ğŸ“‰ 2D dÃ¶ntÃ©si felÃ¼let
    st.subheader("ğŸ“ˆ DÃ¶ntÃ©si felÃ¼let (2D)")
    xx, yy = np.meshgrid(np.linspace(0,1,200), np.linspace(0,1,200))
    grid = np.c_[xx.ravel(), yy.ravel()]
    Z = model.predict(grid).reshape(xx.shape)
    plt.figure(figsize=(4,4))
    plt.contourf(xx, yy, Z, alpha=0.3, cmap="RdBu")
    plt.scatter(X[:,0], X[:,1], c=y, cmap="RdBu", edgecolor="k", s=100)
    plt.title(f"PontossÃ¡g: {acc*100:.1f}%")
    st.pyplot(plt.gcf())

    # ğŸŒ 3D aktivÃ¡ciÃ³
    st.subheader("ğŸŒ Rejtett rÃ©teg aktivÃ¡ciÃ³ (3D)")
    act = model.predict_proba(grid)[:,1].reshape(xx.shape)
    fig3d = go.Figure(data=[go.Surface(z=act, x=xx, y=yy, colorscale="Viridis")])
    fig3d.update_layout(
        scene=dict(xaxis_title="xâ‚", yaxis_title="xâ‚‚", zaxis_title="P(kimenet=1)"),
        margin=dict(l=10,r=10,t=50,b=10),
        height=600)
    st.plotly_chart(fig3d, use_container_width=True)

    # ğŸ“‰ VesztesÃ©ggÃ¶rbe (ha elÃ©rhetÅ‘)
    if hasattr(model, "loss_curve_"):
        st.subheader("ğŸ“‰ VesztesÃ©ggÃ¶rbe")
        fig_loss, ax_loss = plt.subplots()
        ax_loss.plot(model.loss_curve_, label="Log-loss")
        ax_loss.set_xlabel("IterÃ¡ciÃ³")
        ax_loss.set_ylabel("VesztesÃ©g")
        ax_loss.set_title("TanulÃ¡si gÃ¶rbe")
        ax_loss.grid(True)
        st.pyplot(fig_loss)

    # ğŸ“Š SÃºlymÃ¡trix vizualizÃ¡ciÃ³
    st.subheader("ğŸ§® SÃºlymÃ¡trix")
    all_weights = []
    for i, coef in enumerate(model.coefs_):
        flat = coef.flatten()
        for j, val in enumerate(flat):
            all_weights.append((f"W{i+1}_{j+1}", val))
    df_weights = pd.DataFrame(all_weights, columns=["SÃºly", "Ã‰rtÃ©k"])
    st.dataframe(df_weights.set_index("SÃºly").T)

    # ğŸ§© EredmÃ©ny
    st.subheader("ğŸ¯ EredmÃ©nyek")
    st.markdown(f"""
    - HÃ¡lÃ³ struktÃºra: **Inputâ€“{hidden_size}â€“Output**  
    - Solver: **{solver}**  
    - TanulÃ¡si rÃ¡ta: **{learning_rate}**  
    - IterÃ¡ciÃ³: **{model.n_iter_}** / {max_iter}  
    - PontossÃ¡g: **{acc*100:.2f}%**
    """)

    # ğŸ“ CSV export
    st.subheader("ğŸ’¾ SÃºlyok exportÃ¡lÃ¡sa CSV-ben")
    weights_only = np.hstack([coef.flatten() for coef in model.coefs_])
    df_csv = pd.DataFrame(weights_only.reshape(1, -1),
                          columns=[f"w{i}" for i in range(len(weights_only))])
    csv = df_csv.to_csv(index=False).encode('utf-8')
    st.download_button("â¬‡ï¸ SÃºlyok letÃ¶ltÃ©se", data=csv, file_name="xor_weights.csv")

    # ğŸ“˜ TudomÃ¡nyos hÃ¡ttÃ©r â€“ MÃ©lyÃ­tett magyarÃ¡zatokkal
    st.markdown("### ğŸ“™ TudomÃ¡nyos hÃ¡ttÃ©r")

    st.latex(r"""
    \text{XOR kimenet: } y = 
    \begin{cases}
        1 & \text{ha } x_1 \ne x_2 \\
        0 & \text{ha } x_1 = x_2
    \end{cases}
    """)

    st.latex(r"""
    \text{MLP kimenet: } 
    \hat{y} = \sigma\left(W^{(2)} \cdot \sigma(W^{(1)}x + b^{(1)}) + b^{(2)}\right)
    """)

    st.latex(r"""
    \text{AktivÃ¡ciÃ³ (ReLU vagy sigmoid): } 
    \sigma(x) = \frac{1}{1 + e^{-x}} \quad \text{vagy} \quad \max(0, x)
    """)

    st.latex(r"""
    \text{Log-vesztesÃ©g: } 
    L = -\frac{1}{N}\sum_i\left[y_i\log(\hat{y}_i) + (1 - y_i)\log(1 - \hat{y}_i)\right]
    """)

    st.markdown("""
    #### ğŸ”¬ MagyarÃ¡zat

    - A **log-loss** minimalizÃ¡lÃ¡sa sorÃ¡n a modell egyre pontosabban becsÃ¼li a valÃ³szÃ­nÅ±sÃ©geket.
    - Az **egyrÃ©tegÅ± perceptron** nem kÃ©pes megtanulni az XOR nemlineÃ¡ris elvÃ¡lasztÃ¡sÃ¡t.
    - Egy **rejtett rÃ©teg** viszont lehetÅ‘vÃ© teszi a nemlineÃ¡ris dÃ¶ntÃ©si hatÃ¡r megtanulÃ¡sÃ¡t.
    - A **sÃºlyok** elemzÃ©sÃ©vel megfigyelhetÅ‘, hogyan reprezentÃ¡lja a modell a logikai kapcsolatokat.

    #### ğŸ“Œ KÃ¶vetkeztetÃ©s

    Ez a modul vizuÃ¡lisan Ã©s matematikailag is demonstrÃ¡lja, hogyan kÃ©pes egy egyszerÅ± MLP megoldani a hÃ­res **XOR problÃ©mÃ¡t**.
    """)

# ReflectAI kompatibilitÃ¡s
app = run
