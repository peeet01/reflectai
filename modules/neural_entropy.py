import streamlit as st
import numpy as np
import pandas as pd
from scipy.stats import entropy
import matplotlib.pyplot as plt
import plotly.graph_objects as go

# Shannon-entrÃ³pia szÃ¡mÃ­tÃ¡sa
def shannon_entropy(signal, bins):
    hist, _ = np.histogram(signal, bins=bins, density=True)
    hist = hist[hist > 0]
    return entropy(hist, base=2)

# Renyi-entrÃ³pia szÃ¡mÃ­tÃ¡sa
def renyi_entropy(signal, alpha, bins):
    hist, _ = np.histogram(signal, bins=bins, density=True)
    hist = hist[hist > 0]
    if alpha == 1.0:
        return entropy(hist, base=2)
    return 1 / (1 - alpha) * np.log2(np.sum(hist ** alpha))

# Szintetikus jel generÃ¡lÃ¡sa
def generate_signal(kind, length, noise):
    t = np.linspace(0, 4 * np.pi, length)
    if kind == "Szinusz":
        sig = np.sin(t)
    elif kind == "KÃ¡osz (logisztikus)":
        sig = np.zeros(length)
        sig[0] = 0.5
        r = 3.9
        for i in range(1, length):
            sig[i] = r * sig[i - 1] * (1 - sig[i - 1])
    elif kind == "FehÃ©r zaj":
        sig = np.random.randn(length)
    else:
        sig = np.zeros(length)
    return sig + np.random.normal(0, noise, size=length)

def app():
    st.title("ğŸ§  NeurÃ¡lis EntrÃ³pia IdÅ‘sorokon")

    kind = st.selectbox("Jel tÃ­pusa", ["Szinusz", "KÃ¡osz (logisztikus)", "FehÃ©r zaj"])
    noise = st.slider("Zajszint (Ïƒ)", 0.0, 1.0, 0.1, step=0.01)
    signal_len = st.slider("Jel hossza", 200, 5000, 1000, step=100)
    window = st.slider("AblakmÃ©ret", 50, 500, 200, step=10)
    stride = st.slider("LÃ©pÃ©skÃ¶z", 10, 200, 50, step=10)
    bins = st.slider("Hisztogram bin szÃ¡m", 5, 100, 30)
    entropy_type = st.selectbox("EntrÃ³pia tÃ­pusa", ["Shannon", "Renyi"])
    alpha = 1.0
    if entropy_type == "Renyi":
        alpha = st.slider("Renyi Î± paramÃ©ter", 0.1, 5.0, 2.0, step=0.1)

    sig = generate_signal(kind, signal_len, noise)
    entropies = []
    times = []

    for start in range(0, len(sig) - window, stride):
        segment = sig[start:start + window]
        if entropy_type == "Shannon":
            h = shannon_entropy(segment, bins)
        else:
            h = renyi_entropy(segment, alpha, bins)
        entropies.append(h)
        times.append(start)

    # 2D plot
    st.subheader("ğŸ“‰ EntrÃ³pia idÅ‘ben")
    fig, ax = plt.subplots()
    ax.plot(times, entropies, marker='o')
    ax.set_xlabel("IdÅ‘ (mintavÃ©teli index)")
    ax.set_ylabel("EntrÃ³pia (bit)")
    ax.set_title("EntrÃ³pia gÃ¶rbe")
    ax.grid(True)
    st.pyplot(fig)

    # 3D plot
    st.subheader("ğŸŒ 3D entrÃ³piafelÃ¼let")
    x = np.array(times)
    y = np.zeros_like(x)
    z = np.array(entropies)

    fig3d = go.Figure(data=[go.Scatter3d(
        x=x,
        y=y,
        z=z,
        mode='lines+markers',
        marker=dict(size=4, color=z, colorscale='Viridis'),
        line=dict(color='blue', width=2)
    )])
    fig3d.update_layout(scene=dict(
        xaxis_title="IdÅ‘",
        yaxis_title="TÃ­pusindex",
        zaxis_title="EntrÃ³pia (bit)"
    ))
    st.plotly_chart(fig3d, use_container_width=True)

    # Export CSV
    st.subheader("ğŸ“¥ Export")
    df = pd.DataFrame({"index": times, "entropy": entropies})
    csv = df.to_csv(index=False).encode("utf-8")
    st.download_button("LetÃ¶ltÃ©s CSV-ben", data=csv, file_name="entropy_time_series.csv")

    # Matematikai hÃ¡ttÃ©r (egysÃ©ges latex)
    st.markdown(r"""
    ### ğŸ“š Matematikai hÃ¡ttÃ©r

    Az **entrÃ³pia** az informÃ¡ciÃ³elmÃ©let alapfogalma, amely a rendezetlensÃ©g, bizonytalansÃ¡g vagy informÃ¡ciÃ³mennyisÃ©g mÃ©rtÃ©ke egy rendszerben. Matematikailag az entrÃ³piÃ¡t valÃ³szÃ­nÅ±sÃ©gi eloszlÃ¡sokhoz rendelhetjÃ¼k.

    #### Shannon-entrÃ³pia

    A Shannon-entrÃ³pia az Ã¡tlagos informÃ¡ciÃ³tartalmat mÃ©ri:

    \[
    H = -\sum_i p_i \log_2 p_i
    \]

    ahol î€p_iî€ az adott Ã¡llapot elÅ‘fordulÃ¡sÃ¡nak valÃ³szÃ­nÅ±sÃ©ge. Ez az entrÃ³pia Ã©rtÃ©k î€0î€ (teljes rendezettsÃ©g) Ã©s î€\log_2 Nî€ (egyenletes eloszlÃ¡s) kÃ¶zÃ¶tt vÃ¡ltozik.

    #### RÃ©nyi-entrÃ³pia

    A RÃ©nyi-entrÃ³pia Ã¡ltalÃ¡nosÃ­tja a Shannon-entrÃ³piÃ¡t egy valÃ³s paramÃ©ter, î€\alphaî€ segÃ­tsÃ©gÃ©vel:

    \[
    H_\alpha = \frac{1}{1 - \alpha} \log_2 \left( \sum_i p_i^\alpha \right), \quad \alpha > 0, \ \alpha \ne 1
    \]

    - Ha î€\alpha \to 1î€, akkor î€H_\alpha \to Hî€ (Shannon-entrÃ³pia).
    - Kis î€\alphaî€ esetÃ©n Ã©rzÃ©kenyebb a ritka esemÃ©nyekre, nagy î€\alphaî€ esetÃ©n a dominÃ¡ns mintÃ¡zatokra.

    ---

    ### ğŸ§  AlkalmazÃ¡s az idegtudomÃ¡nyban

    Az entrÃ³pia mÃ©rÃ©se fontos eszkÃ¶z az **agyi aktivitÃ¡s komplexitÃ¡sÃ¡nak** vagy **rendezettsÃ©gÃ©nek** vizsgÃ¡latÃ¡ban.

    - A **neuronÃ¡lis jelek entrÃ³piÃ¡ja** Ã¶sszefÃ¼gg az informÃ¡ciÃ³feldolgozÃ¡s hatÃ©konysÃ¡gÃ¡val.
    - **Alacsony entrÃ³pia** â†’ nagyfokÃº szinkronitÃ¡s, ismÃ©tlÅ‘dÅ‘ mintÃ¡zatok (pl. epilepsziÃ¡s rohamokban).
    - **Magas entrÃ³pia** â†’ vÃ¡ltozatos, adaptÃ­v dinamika (pl. tanulÃ¡s, figyelem sorÃ¡n).
    - A RÃ©nyi-entrÃ³pia paramÃ©tereivel finomhangolhatÃ³ a ritka vagy dominÃ¡ns mintÃ¡k Ã©rzÃ©kenysÃ©ge.

    Ez az entrÃ³piaalapÃº elemzÃ©s kÃ¼lÃ¶nÃ¶sen hasznos **Echo State Network (ESN)** kimenetek, EEG szignÃ¡lok vagy bÃ¡rmilyen nemlineÃ¡ris idÅ‘jel komplexitÃ¡sÃ¡nak kvantifikÃ¡lÃ¡sÃ¡ra.
    """)
    # Fontos: csak akkor fut le, ha lokÃ¡lisan teszteled (a deployhoz NE Ã­rd be)
    # if __name__ == "__main__":
    #     app()
