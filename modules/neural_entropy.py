import streamlit as st
import numpy as np
import pandas as pd
from scipy.stats import entropy
import matplotlib.pyplot as plt
import plotly.graph_objects as go

# Shannon-entrÃ³pia szÃ¡mÃ­tÃ¡sa
def shannon_entropy(signal, bins):
    hist, _ = np.histogram(signal, bins=bins, density=True)
    hist = hist[hist > 0]
    return entropy(hist, base=2)

# Renyi-entrÃ³pia szÃ¡mÃ­tÃ¡sa
def renyi_entropy(signal, alpha, bins):
    hist, _ = np.histogram(signal, bins=bins, density=True)
    hist = hist[hist > 0]
    if alpha == 1.0:
        return entropy(hist, base=2)
    return 1 / (1 - alpha) * np.log2(np.sum(hist ** alpha))

# Szintetikus jel generÃ¡lÃ¡sa
def generate_signal(kind, length, noise):
    t = np.linspace(0, 4 * np.pi, length)
    if kind == "Szinusz":
        sig = np.sin(t)
    elif kind == "KÃ¡osz (logisztikus)":
        sig = np.zeros(length)
        sig[0] = 0.5
        r = 3.9
        for i in range(1, length):
            sig[i] = r * sig[i - 1] * (1 - sig[i - 1])
    elif kind == "FehÃ©r zaj":
        sig = np.random.randn(length)
    else:
        sig = np.zeros(length)
    return sig + np.random.normal(0, noise, size=length)

def app():
    st.title("ğŸ§  NeurÃ¡lis EntrÃ³pia IdÅ‘sorokon")

    kind = st.selectbox("Jel tÃ­pusa", ["Szinusz", "KÃ¡osz (logisztikus)", "FehÃ©r zaj"])
    noise = st.slider("Zajszint (Ïƒ)", 0.0, 1.0, 0.1, step=0.01)
    signal_len = st.slider("Jel hossza", 200, 5000, 1000, step=100)
    window = st.slider("AblakmÃ©ret", 50, 500, 200, step=10)
    stride = st.slider("LÃ©pÃ©skÃ¶z", 10, 200, 50, step=10)
    bins = st.slider("Hisztogram bin szÃ¡m", 5, 100, 30)
    entropy_type = st.selectbox("EntrÃ³pia tÃ­pusa", ["Shannon", "Renyi"])
    alpha = 1.0
    if entropy_type == "Renyi":
        alpha = st.slider("Renyi Î± paramÃ©ter", 0.1, 5.0, 2.0, step=0.1)

    sig = generate_signal(kind, signal_len, noise)
    entropies = []
    times = []

    for start in range(0, len(sig) - window, stride):
        segment = sig[start:start + window]
        if entropy_type == "Shannon":
            h = shannon_entropy(segment, bins)
        else:
            h = renyi_entropy(segment, alpha, bins)
        entropies.append(h)
        times.append(start)

    # 2D plot
    st.subheader("ğŸ“‰ EntrÃ³pia idÅ‘ben")
    fig, ax = plt.subplots()
    ax.plot(times, entropies, marker='o')
    ax.set_xlabel("IdÅ‘ (mintavÃ©teli index)")
    ax.set_ylabel("EntrÃ³pia (bit)")
    ax.set_title("EntrÃ³pia gÃ¶rbe")
    ax.grid(True)
    st.pyplot(fig)

    # 3D plot
    st.subheader("ğŸŒ 3D entrÃ³piafelÃ¼let")
    x = np.array(times)
    y = np.zeros_like(x)
    z = np.array(entropies)

    fig3d = go.Figure(data=[go.Scatter3d(
        x=x,
        y=y,
        z=z,
        mode='lines+markers',
        marker=dict(size=4, color=z, colorscale='Viridis'),
        line=dict(color='blue', width=2)
    )])
    fig3d.update_layout(scene=dict(
        xaxis_title="IdÅ‘",
        yaxis_title="TÃ­pusindex",
        zaxis_title="EntrÃ³pia (bit)"
    ))
    st.plotly_chart(fig3d, use_container_width=True)

    # Export CSV
    st.subheader("ğŸ“¥ Export")
    df = pd.DataFrame({"index": times, "entropy": entropies})
    csv = df.to_csv(index=False).encode("utf-8")
    st.download_button("LetÃ¶ltÃ©s CSV-ben", data=csv, file_name="entropy_time_series.csv")

    # Matematikai hÃ¡ttÃ©r (egysÃ©ges latex)
    st.markdown(r"""
    ### ğŸ“š Matematikai hÃ¡ttÃ©r

    Az **entrÃ³pia** egy mÃ©rtÃ©k annak, mennyi informÃ¡ciÃ³t hordoz egy adott eloszlÃ¡s â€“ vagyis mekkora a rendezetlensÃ©g.

    ---

    #### Shannon-entrÃ³pia

    A **Shannon-entrÃ³pia** az informÃ¡ciÃ³elmÃ©let alapmennyisÃ©ge, az alÃ¡bbi kÃ©plettel definiÃ¡lhatÃ³:

    \[
    H = -\sum_i p_i \log_2 p_i
    \]

    ahol î€p_iî€ az adott Ã¡llapot elÅ‘fordulÃ¡si valÃ³szÃ­nÅ±sÃ©ge.

    - Ha az eloszlÃ¡s koncentrÃ¡lt (egy valÃ³szÃ­nÅ±sÃ©g dominÃ¡l), î€H \to 0î€
    - Ha minden esemÃ©ny egyenlÅ‘ valÃ³szÃ­nÅ±sÃ©gÅ±: î€H \to \log_2 Nî€

    ---

    #### RÃ©nyi-entrÃ³pia

    A **RÃ©nyi-entrÃ³pia** a Shannon-entrÃ³pia Ã¡ltalÃ¡nosÃ­tÃ¡sa egy î€\alphaî€ paramÃ©ter segÃ­tsÃ©gÃ©vel:

    \[
    H_\alpha = \frac{1}{1 - \alpha} \log_2 \left( \sum_i p_i^\alpha \right), \quad \alpha > 0, \ \alpha \ne 1
    \]

    - Ha î€\alpha \to 1î€, akkor î€H_\alpha \to Hî€ (Shannon-entrÃ³pia)
    - Kis î€\alphaî€: Ã©rzÃ©keny ritka esemÃ©nyekre
    - Nagy î€\alphaî€: a dominÃ¡ns mintÃ¡zatokra koncentrÃ¡l

    ---

    ### ğŸ§  AlkalmazÃ¡s idegtudomÃ¡nyban

    Az entrÃ³piÃ¡k elemzÃ©se segÃ­thet megÃ©rteni a neurÃ¡lis rendszerek **komplexitÃ¡sÃ¡t Ã©s rendezettsÃ©gÃ©t**:

    - **Alacsony entrÃ³pia** â†’ ismÃ©tlÅ‘dÅ‘, kiszÃ¡mÃ­thatÃ³ dinamika (pl. epilepsziÃ¡s mintÃ¡zat)
    - **Magas entrÃ³pia** â†’ vÃ¡ltozatos, adaptÃ­v aktivitÃ¡s (pl. figyelem, tanulÃ¡s)
    - **RÃ©nyi-entrÃ³pia** finomhangolhatÃ³ a kÃ¼lÃ¶nbÃ¶zÅ‘ dinamikÃ¡kra, Ã©rzÃ©kenyen jelzi a szinkronitÃ¡s vagy a kÃ¡osz jelenlÃ©tÃ©t

    HasznÃ¡lhatÃ³ pl. **ESN kimenetek**, **EEG jelek** vagy mÃ¡s idÅ‘alapÃº idegrendszeri szignÃ¡lok rendezettsÃ©gÃ©nek becslÃ©sÃ©re.
    """)
    # Fontos: csak akkor fut le, ha lokÃ¡lisan teszteled (a deployhoz NE Ã­rd be)
    # if __name__ == "__main__":
    #     app()
